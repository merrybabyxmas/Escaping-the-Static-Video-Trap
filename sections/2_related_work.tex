\section{Related Work}
\label{sec:related}

\subsection{Text-to-Video Generation Architectures}
The rapid evolution of diffusion models has bifurcated the field of video generation into two distinct architectural paradigms: \textbf{Foundation T2V Models} and \textbf{Specialized T2MSV Frameworks}. Foundation models, such as CogVideoX~\cite{yang2024cogvideox}, LTX-Video~\cite{HaCohen2024LTXVideo}, and Stable Video Diffusion (SVD)~\cite{blattmann2023stable}, scale up massive video datasets to learn raw spatiotemporal priors. While these models excel at rendering high-fidelity, single-shot motions, they critically lack the explicit control mechanisms required for complex, multi-shot narrative sequencing. To address this limitation, Specialized T2MSV Frameworks aim to guide these base models toward narrative generation via complex inference pipelines. Methods such as StoryDiffusion~\cite{zhou2024storydiffusion} leverage consistent self-attention modules to enforce character tracking across frames, whereas frameworks like DirecT2V~\cite{hong2023large} and Mora~\cite{yuan2024mora} utilize LLM-driven multi-agent systems to explicitly enforce shot boundaries and camera directions. Alternatively, tuning-free inference methods such as FreeNoise~\cite{qiu2023freenoise} attempt to extend video length via noise rescheduling techniques. Despite these sophisticated architectural and inference-time interventions, our empirical evaluation reveals that both paradigms still suffer profoundly from unintended prompt bleeding and catastrophic morphing artifacts during multi-shot generation.

\subsection{Limitations of Video Generative Metrics}
Current evaluation suites for video generation, such as VBench~\cite{huang2023vbench}, rely on a taxonomy of metrics that are fundamentally misaligned with the cinematic grammar required by T2MSV, leading to systemic evaluation blind spots. Primarily, this introduces the \textbf{Global Similarity Paradox}. Evaluating character consistency using holistic image similarity leads to a contradictory assessment: if a model correctly executes a drastic background shift as instructed, its holistic structural similarity artificially drops, penalizing the model for strictly following semantic instructions. We resolve this by decoupling the foreground subject via precise DINOv2~\cite{oquab2023dinov2} masking. Furthermore, traditional temporal coherence metrics suffer from a \textbf{Uniform Continuity Bias}. These metrics treat all inter-frame differences as generative errors, despite the fact that intentional "cuts" are a mandatory component of cinematic storytelling. Legacy metrics inherently treat these sharp cinematic transitions as visual artifacts. To counteract this, our framework evaluates \textit{Cut-Transition Sharpness} using LPIPS~\cite{zhang2018perceptual}, demanding high perceptual divergence exclusively when a shot transition is prompted. Finally, while recent frameworks propose using Vision-Language Models (VLMs) like GPT-4V for narrative QA, VLMs struggle to robustly detect pixel-level \textit{morphing}â€”where objects unnaturally melt into one another instead of cutting cleanly. To overcome these limitations, our \textbf{Diagonal Semantic Alignment (DSA)} metric leverages robust CLIP~\cite{radford2021learning} feature matrices to mathematically guarantee shot-level independence and instruction adherence.
