\section{Related Work}
\label{sec:related}

\subsection{Text-to-Video Generation Architectures}
The rapid evolution of diffusion models has bifurcated the field of video generation into two distinct architectural paradigms: \textbf{Foundation T2V Models} and \textbf{Specialized T2MSV Frameworks}. Foundation models, such as CogVideoX~\cite{yang2024cogvideox}, LTX-Video~\cite{HaCohen2024LTXVideo}, and Stable Video Diffusion (SVD)~\cite{blattmann2023stable}, scale up massive video datasets to learn raw spatiotemporal priors. While these models excel at rendering high-fidelity, single-shot motions, they critically lack the explicit control mechanisms required for complex, multi-shot narrative sequencing. To address this limitation, Specialized T2MSV Frameworks aim to guide these base models toward narrative generation via complex inference pipelines. Methods such as StoryDiffusion~\cite{zhou2024storydiffusion} leverage consistent self-attention modules to enforce character tracking across frames, whereas frameworks like DirecT2V~\cite{hong2023large} and Mora~\cite{yuan2024mora} utilize multi-agent systems to explicitly enforce shot boundaries and camera directions. Alternatively, tuning-free inference methods such as FreeNoise~\cite{qiu2023freenoise} and AnimateDiff~\cite{guo2023animatediff} attempt to extend video length or animate personalized models. Despite these sophisticated architectural and inference-time interventions, our empirical evaluation reveals that both paradigms still suffer profoundly from unintended prompt bleeding and catastrophic morphing artifacts during multi-shot generation.

\subsection{The Flaws of Current T2MSV Evaluation}
Current evaluation suites for video generation, such as VBench~\cite{huang2023vbench}, rely on a taxonomy of metrics that are fundamentally misaligned with the cinematic grammar required by T2MSV, leading to systemic evaluation blind spots.

\noindent\textbf{The Middle-Frame Fallacy.} Many current studies evaluate identity and background consistency by sampling merely a single middle frame per shot. This approach is critically flawed as it fails to penalize temporal artifacts like morphing, where objects unnaturally melt into one another instead of cutting cleanly. Our framework overcomes this by employing a 4D decoupled evaluation pipeline that assesses consistency across all generated frames.

\noindent\textbf{The Uniform Continuity Bias.} Traditional temporal coherence metrics assume that lower frame-wise distance (e.g., LPIPS~\cite{zhang2018perceptual}) is always better. This bias unfairly penalizes intentional, cinematic cuts, treating sharp transitions as visual artifacts. We resolve this by introducing \textit{Cut-Transition Sharpness}, which demands high perceptual divergence exclusively when a shot transition is prompted, effectively validating the model's ability to execute a clean break.

\noindent\textbf{Absolute vs. Contrastive Alignment.} Existing metrics rely on absolute CLIP~\cite{radford2021learning} scores to measure text-video alignment. However, absolute scores are blind to \textbf{Context Bleeding}â€”where a model hallucinates elements from previous or future prompts into the current shot. Ironically, naive baselines that concatenate independent shots often achieve higher absolute alignment than end-to-end models. Our proposed \textbf{Diagonal Semantic Alignment (DSA)} shifts the paradigm to contrastive instruction isolation. By leveraging a column-wise softmax normalization, DSA forces a zero-sum game that explicitly demands exclusivity and penalizes context bleeding, ensuring that instructions are isolated and executed only when intended.

\noindent\textbf{The Global Similarity Paradox.} Evaluating character consistency using holistic image similarity leads to a contradictory assessment: if a model correctly executes a drastic background shift as instructed, its holistic structural similarity artificially drops, penalizing the model for strictly following semantic instructions. We resolve this by decoupling the foreground subject from the background environment via precise DINOv2~\cite{oquab2023dinov2} masking, allowing for a more granular and accurate diagnosis of generative successes and failures.
