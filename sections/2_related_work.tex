\section{Related Work}
\label{sec:related}

\subsection{Specialized T2MSV Frameworks vs. Foundation Models}
The rapid evolution of diffusion models has bifurcated the field. On one hand, \textbf{Foundation T2V Models} (e.g., CogVideoX~\cite{yang2024cogvideox}, LTX-Video~\cite{HaCohen2024LTXVideo}, SVD~\cite{blattmann2023stable}) scale up massive datasets to learn raw spatiotemporal priors. However, they lack explicit multi-shot conditioning. On the other hand, \textbf{T2MSV-Specific Frameworks} aim to guide these base models for narrative generation. Methods like StoryDiffusion~\cite{zhou2024storydiffusion} leverage consistent self-attention for character tracking, while DirecT2V~\cite{hong2023large} and Mora~\cite{yuan2024mora} utilize LLM-driven multi-agent pipelines to enforce shot boundaries. FreeNoise~\cite{qiu2023freenoise} attempts tuning-free long video extension via noise rescheduling. Despite these efforts, our evaluation reveals they still suffer from \textit{prompt bleeding} and morphing artifacts.

\subsection{The Flaws of Legacy Evaluation in T2MSV}
Current benchmark suites like VBench~\cite{huang2023vbench} rely on metrics fundamentally misaligned with the cinematic grammar of T2MSV:

\textbf{1. The Global Similarity Paradox:} Evaluating character consistency using holistic image similarity (as in StoryDiffusion) leads to a paradox. If a model correctly executes a drastic background shift, the holistic similarity artificially drops. Models are thus penalized for following dynamic instructions. We resolve this by decoupling the subject via DINOv2~\cite{oquab2023dinov2} masking.

\textbf{2. The Uniform Continuity Bias:} Traditional temporal coherence metrics treat all inter-frame differences as errors. In T2MSV, intentional "cuts" are mandatory. Legacy metrics treat these sharp cinematic transitions as visual artifacts. Our framework evaluates \textit{Cut-Transition Sharpness} using LPIPS~\cite{zhang2018perceptual}, demanding high sharpness during semantic shifts but smooth transitions during continuous camera motions.

\textbf{3. VLM-as-a-Judge Limits:} While some frameworks use GPT-4V for narrative QA, VLMs struggle to detect pixel-level \textit{morphing}â€”where objects melt into one another instead of cutting cleanly. To overcome this, our \textbf{DSA} metric leverages CLIP~\cite{radford2021learning} feature matrices to mathematically guarantee shot-level independence.
