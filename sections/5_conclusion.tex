\section{Conclusion}
\label{sec:conclusion}

In this paper, we have systematically exposed the fundamental flaws of current Text-to-Multi-Shot Video (T2MSV) evaluation methodologies. By introducing the \textbf{DIAL} (Diagonal Instruction ALignment) benchmark and its two orthogonal tracks—Track S (Semantic Leap) and Track M (Motion Continuity)—we have provided a rigorous diagnostic framework for identifying critical failure modes: the \textbf{Static Trap}, \textbf{Context Bleeding}, and \textbf{Identity Amnesia}.

Central to our work is the formulation of \textbf{Diagonal Semantic Alignment (DSA)}, which shifts the paradigm from absolute alignment to contrastive instruction isolation. By leveraging a mathematically robust, zero-sum formulation, DSA provides the first objective mechanism for penalizing instruction-agnostic generation and ensuring shot-level semantic independence. Our exhaustive evaluation of state-of-the-art models uncovers a pervasive \textbf{``Double-Kill'' dilemma}, where current architectures struggle to balance identity preservation with narrative dynamism.

Ultimately, DIAL serves as more than a leaderboard; it is a critical diagnostic foundation for the next generation of video generative models. Our findings demonstrate that current monolithic architectures are structurally insufficient for mastering complex multi-shot narratives. The community must pivot toward \textbf{``Decoupled Dynamics''}—architectures that explicitly isolate identity-preserving priors from narrative-driven motion priors. We hope this benchmark will catalyze research into more robust, spatiotemporally decoupled video priors, paving the way for truly cinematic, instruction-aligned video synthesis.
