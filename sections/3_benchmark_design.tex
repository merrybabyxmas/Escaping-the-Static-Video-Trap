\section{Proposed Evaluation Framework}
\label{sec:metrics}

We argue that a robust multi-shot video metric must be \textit{decoupled} and \textit{adversarial}. We propose three key metrics to break the Static Video Trap.

\textbf{Metric 1: Decoupled Subject-Background Consistency.} 
Standard CLIP-based consistency fails because the background dominates the embedding. We utilize Grounding-DINO to isolate the subject. We define Subject Consistency ($\mathcal{C}_s$) as the mean DINOv2 cosine similarity of cropped subjects across shots, and Background Diversity ($\mathcal{D}_b$) as the variance of the remaining pixel-space embeddings.

\textbf{Metric 2: Diagonal Semantic Alignment (DSA).}
For a multi-shot prompt sequence $P = \{P_1, \dots, P_K\}$, a video $V$ should satisfy $V_i \approx P_i$. We construct a $K \times K$ alignment matrix $A_{i,j} = \text{sim}(V_i, P_j)$. The DSA score is defined as the ratio of the diagonal energy to the total matrix energy. A low DSA score indicates that the model is "leaking" semantics from previous shots into current ones—a hallmark of the Static Video Trap.

\textbf{Metric 3: Cut-Transition Sharpness (CTS).}
Current models often transition between shots using "morphing" artifacts—unrealistic pixel-blending that minimizes LPIPS distance. CTS measures the kurtosis of the frame-to-frame LPIPS distance, penalizing models that fail to produce clean, distinct cinematic cuts.
