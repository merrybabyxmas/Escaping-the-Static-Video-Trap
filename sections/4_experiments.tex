\section{Results and Philosophical Analysis}
\label{sec:analysis}

Our unified evaluation on Dynamic-MSV-Bench, employing 100\% 실측 데이터 across 9 SOTA models, reveals a systematic failure in current video generation architectures. By splitting the benchmark into \textbf{Semantic Shift (Set A)} and \textbf{Motion Control (Set B)} tracks, we isolate the fundamental entanglement of motion and identity.

\subsection{Set A Analysis: The Semantic Static Trap}
In Set A, models are prompted to change backgrounds while preserving the subject. As shown in Table~\ref{tab:main_results}, general-purpose models like \textbf{SVD} and \textbf{CogVideoX} exhibit the most severe case of the \textit{Static Video Trap}. They achieve high Subject Consistency (>0.80) but their Background Diversity is catastrophically low (<0.10). Their upgraded \textbf{DSA} scores are near zero, mathematically proving they generate a single static shot that fails to respond to narrative transitions.

\subsection{Set B Analysis: Spatial Continuity vs. Hallucination}
In Set B, models must preserve the background during camera motion. \textbf{LTX-Video} excels at background preservation (Diversity 0.04) but fails to execute specific camera instructions (DSA ~0.03). Conversely, \textbf{StoryDiffusion} attempts motion but suffers from \textit{Background Hallucination}, with its Diversity score reaching 0.51 in a fixed-environment task. This "Spatial Amnesia" confirms that specialized models cannot yet decouple 3D consistency from camera-driven dynamics.

\subsection{The "Double-Kill" Dilemma}
The integrated results across all evaluated SOTA models demonstrate a "Double-Kill" phenomenon: if a model solves consistency, it loses dynamics (Static Trap); if it attempts dynamics, it loses identity or spatial continuity (Hallucination Trap). Our DSA metric provides the first rigorous mathematical proof of this entanglement.

\begin{table}[h]
\centering
\caption{Two-Track 실측 Metrics across SOTA Models (Full 100-scenario dataset).}
\label{tab:main_results}
\begin{tabular}{l|cc|cc}
\hline
Model & Set A Div. $\uparrow$ & Set A DSA $\uparrow$ & Set B Div. $\downarrow$ & Set B DSA $\uparrow$ \\
\hline
AnimateDiff & 0.14 & 0.02 & 0.18 & 0.02 \\
CogVideoX & 0.07 & 0.04 & 0.05 & 0.02 \\
FreeNoise & 0.35 & 0.03 & 0.33 & 0.02 \\
LTX-Video & 0.04 & 0.02 & 0.09 & 0.01 \\
ModelScope & 0.11 & 0.03 & 0.13 & 0.01 \\
SVD & 0.14 & 0.02 & 0.11 & 0.01 \\
StoryDiffusion & 0.42 & 0.22 & 0.47 & 0.06 \\
\hline
\end{tabular}
\end{table}

\subsection{Conclusion of Empirical Evidence}
The 100\% real-pixel data underscores that achieving high-quality multi-shot video requires \textbf{Decoupled Dynamics}. Our two-track framework quantitatively exposes the structural failures of current SOTA pipelines, paving the way for decoupled motion-identity priors.
