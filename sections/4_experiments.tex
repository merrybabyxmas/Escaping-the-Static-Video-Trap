\section{Experiments and Unveiling the Blind Spots}
\label{sec:experiments}

Our evaluation on Dynamic-MSV-Bench reveals a systematic failure in current video generation architectures. To properly diagnose these failures, we categorized the evaluated baselines into two distinct groups: \textbf{Group 1: Foundation T2V Models} and \textbf{Group 2: Specialized T2MSV Frameworks}.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/radar_chart_set_a.pdf}
        \caption{Track S: Semantic Leap}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/radar_chart_set_b.pdf}
        \caption{Track M: Motion Continuity}
    \end{subfigure}
    \caption{Radar Charts of Model Performance. In \textbf{Track S}, an ideal model would form a large diamond. In \textbf{Track M}, we plot inverted axes for Diversity and Sharpness ($1-x$) such that a large area consistently represents high performance.}
    \label{fig:radar}
\end{figure*}

\begin{table*}[t!]
\centering
\caption{Main Results on \textbf{Track S (Semantic Leap)} and \textbf{Track M (Motion Continuity)}. Track S demands narrative-driven dynamics ($\uparrow$ is better for all). Track M demands spatial stability ($\downarrow$ is better for Diversity and Sharpness). Best in \textbf{bold}.}
\label{tab:main_results}
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{ll|cccc|cccc}
\toprule
& & \multicolumn{4}{c|}{\textbf{Track S (Semantic Leap)}} & \multicolumn{4}{c}{\textbf{Track M (Motion Continuity)}} \\
\textbf{Category} & \textbf{Method} & \textbf{Cons.} $\uparrow$ & \textbf{Div.} $\uparrow$ & \textbf{DSA} $\uparrow$ & \textbf{Sharp.} $\uparrow$ & \textbf{Cons.} $\uparrow$ & \textbf{Div.} $\downarrow$ & \textbf{DSA} $\uparrow$ & \textbf{Sharp.} $\downarrow$ \\
\midrule
Foundation & CogVideoX & 0.902 & 0.073 & 0.045 & 0.262 & \textbf{0.939} & \textbf{0.046} & 0.020 & 0.227 \\
Foundation & LTX-Video & \textbf{0.951} & 0.037 & 0.024 & 0.132 & 0.885 & 0.086 & 0.010 & 0.296 \\
Foundation & ModelScope & 0.851 & 0.112 & 0.031 & 0.352 & 0.822 & 0.134 & 0.014 & 0.385 \\
Foundation & SVD & 0.816 & 0.138 & 0.020 & 0.102 & 0.852 & 0.111 & 0.012 & \textbf{0.064} \\
Framework & AnimateDiff & 0.816 & 0.138 & 0.023 & 0.485 & 0.765 & 0.176 & 0.017 & 0.530 \\
Framework & FreeNoise & 0.528 & 0.354 & 0.032 & \textbf{0.893} & 0.565 & 0.326 & 0.020 & 0.808 \\
Framework & StoryDiffusion & 0.443 & \textbf{0.418} & \textbf{0.222} & 0.702 & 0.377 & 0.467 & \textbf{0.060} & 0.710 \\
\bottomrule
\end{tabular}}
\end{table*}

\subsection{Group 1: Foundation T2V Models and the Static Trap}
Foundation models (e.g., CogVideoX, LTX-Video, SVD) exhibit the raw limits of spatiotemporal priors. They achieve high Subject Consistency (>0.80) but catastrophically low Background Diversity (<0.10) in Track S. More importantly, their \textbf{DSA} scores remain near $0.02$, proving they generate a single static scene regardless of narrative instructions. This results in the uniform probability distribution seen in Fig.~\ref{fig:dsa_heatmaps}(A).

\subsection{Group 2: T2MSV Frameworks and the Trade-off Dilemma}
Specialized frameworks successfully break out of the Static Trap, achieving higher Background Diversity in Track S. However, this dynamism comes at the severe cost of identity preservation. Their Subject Consistency plummets, leading to \textit{Identity Amnesia}. This is clearly visualized in our comprehensive performance landscape in Figure~\ref{fig:scatter}. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/rich_tradeoff_scatter.pdf}
    \caption{The comprehensive performance landscape across Track S and Track M. The plots explicitly show the relationship between Subject Consistency, Background Diversity, and our proposed Diagonal Semantic Alignment (DSA). Current models are completely missing from the "Ideal Decoupled" goal (high consistency, high DSA).}
    \label{fig:scatter}
\end{figure*}

\subsection{The "Double-Kill" and the Context Paradox}
Our evaluation exposes a fascinating paradox: a metric's value is deeply context-dependent. For instance, CogVideoX exhibits exceptionally low Background Diversity. In Track M, traditional evaluators might misinterpret this as excellent spatial continuity. However, our cross-scenario analysis reveals the truth: the same model exhibits the same low diversity in Track S, proving it is merely trapped in static generation. Without the dual-track perspective of Track S and Track M, this critical failure would remain masked.
