\section{Introduction}
\label{sec:intro}

The field of video generation has transitioned from synthesizing blurry, single-shot clips to attempting complex, cinematic multi-shot narratives (Text-to-Multi-Shot Video, T2MSV). However, evaluation methodologies remain anchored in a single-shot mindset. Existing metrics, such as Fr√©chet Video Distance (FVD) or frame-wise CLIP similarity, evaluate videos holistically. In multi-shot environments, this holistic approach creates a fatal vulnerability: it inadvertently rewards models that generate entirely static, unchanging videos, even when prompted to execute dynamic scene transitions.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/fig1.png}
    \caption{The dilemma of current T2MSV models. Existing models typically fall into one of two failure modes given multi-shot prompts requiring semantic shifts: (A) The "Static Trap," where models maintain high consistency scores by generating near-static imagery, ignoring background change instructions. (B) "Amnesia," where models successfully change backgrounds but fail to preserve subject identity. Our benchmark is the first to quantify these distinct failure modes decoupling identity from dynamics.}
    \label{fig:teaser}
\end{figure}

We define this phenomenon as the \textbf{Static Video Trap}. When a prompt demands a sequence jumping from a "rainforest" to "outer space," a model that stubbornly remains in the rainforest will score artificially high on traditional temporal consistency metrics. Conversely, models attempting to dynamically execute the transition often suffer from \textbf{Identity Amnesia}, losing the features of the main subject.

In this work, we argue that T2MSV requires the precise management of \textit{intended discontinuities}. We introduce \textbf{Dynamic-MSV-Bench}, a comprehensive evaluation framework that disentangles subject identity from environmental dynamics. Our benchmark splits evaluation into two critical tracks: \textbf{Semantic Shift} (Track A), which demands high dynamics and high identity, and \textbf{Motion Control} (Track B), which demands high identity but fixed spatial backgrounds.

Our core contributions are:
\begin{itemize}
    \item We expose the \textbf{Static Trap} and \textbf{Amnesia Trap}, highlighting the flaws of legacy temporal metrics that penalize intended cinematic cuts and semantic shifts.
    \item We propose a 4D decoupled evaluation pipeline, introducing \textbf{Diagonal Semantic Alignment (DSA)}, a mathematically rigorous metric using column-wise softmax normalization that assigns a strict zero score to static, instruction-ignoring videos.
    \item We conduct an exhaustive evaluation of 9 SOTA models, categorically demonstrating that neither Foundation T2V models nor specialized T2MSV inference frameworks have solved the fundamental "Double-Kill" dilemma of multi-shot generation.
\end{itemize}
