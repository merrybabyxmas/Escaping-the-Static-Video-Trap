\section{Introduction}
\label{sec:intro}

The field of generative video AI is rapidly transitioning from synthesizing blurry, single-shot clips to attempting complex, cinematic multi-shot narratives, a paradigm shift we define as \textbf{Text-to-Multi-Shot Video (T2MSV)}. This evolution promises to revolutionize digital storytelling, enabling the creation of coherent films and dynamic visual narratives from simple textual sequences. However, as the complexity of the generated content increases, our ability to evaluate it has stagnated. Current evaluation methodologies remain deeply anchored in a single-shot mindset, relying on metrics that fail to account for the unique spatiotemporal challenges of multi-shot synthesis.

Existing quality and consistency metrics, such as Fréchet Video Distance (FVD)~\cite{blattmann2023stable} or frame-wise CLIP similarity~\cite{radford2021learning}, evaluate generated videos holistically. In the context of T2MSV, this holistic approach introduces a critical structural vulnerability: it inadvertently rewards generative models that produce entirely static, unchanging videos, even when those models are explicitly prompted to execute highly dynamic scene transitions. We define this pervasive failure phenomenon as the \textbf{Static Video Trap}. For example, if a model is prompted with a sequence demanding a narrative leap—such as jumping from a ``dense rainforest'' to ``deep outer space''—a model that stubbornly generates a continuous rainforest scene will score artificially high on traditional temporal consistency metrics. This creates the \textbf{Global Similarity Paradox}, where conventional metrics inherently reward the failure to transition, thereby masking a fundamental inability to follow complex shot-level semantic instructions.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig1_overview.pdf}
    \caption{Overview of the \textbf{DIAL} Benchmark and the \textbf{DSA} Pipeline. (A) Current models exploit traditional consistency metrics via the \textbf{Static Trap} or suffer from \textbf{Identity Amnesia} and \textbf{Context Bleeding}. (B) We propose \textbf{Diagonal Semantic Alignment (DSA)}, which utilizes a column-wise softmax to mathematically guarantee instruction isolation. (C) DIAL correctly penalizes failing models, uncovering the structural ``Double-Kill'' dilemma.}
    \label{fig:overview}
\end{figure*}

Furthermore, current evaluation paradigms are blind to \textbf{Context Bleeding}, where semantic elements from one prompt erroneously persist into subsequent shots. This failure is often obscured by the use of absolute CLIP scores, which measure overall alignment but fail to quantify the \textit{isolation} of instructions. Ironically, we observe that naive baselines—such as concatenating independently generated shots—can achieve higher text alignment scores than sophisticated end-to-end models, a paradox that further underscores the need for a contrastive evaluation framework. When models do attempt to escape the Static Trap by executing dynamic transitions, they frequently succumb to \textbf{Identity Amnesia}, wherein the model loses the fine-grained visual features of the main subject across shots. This suggests a fundamental \textbf{``Double-Kill'' dilemma} in current monolithic architectures: they can either preserve identity at the expense of dynamics or prioritize dynamics at the expense of identity, but they struggle to achieve both.

In this work, we argue that T2MSV requires more than mere alignment; it demands \textbf{Temporal Instruction Isolation}—the ability to execute a specific prompt exclusively at a specific time while actively rejecting adjacent contextual noise. To address this, we introduce \textbf{DIAL} (Diagonal Instruction ALignment), a comprehensive evaluation benchmark designed to diagnose these exact capabilities through a 4D decoupled evaluation framework. Our benchmark categorizes evaluation into two orthogonal tracks: \textbf{Track S (Semantic Leap)} tests narrative diversity by requiring radical environment changes while strictly preserving the subject, and \textbf{Track M (Motion Continuity)} evaluates spatial integrity, where the background must remain consistent while the camera executes specific physical motions.

To quantitatively support this taxonomy, we propose a 4D decoupled evaluation pipeline assessing subject identity, background dynamics, cut sharpness, and instruction alignment. Central to this pipeline is \textbf{Diagonal Semantic Alignment (DSA)}, a novel metric that applies column-wise softmax normalization to shot-prompt similarity matrices. This formulation forces a zero-sum game, demanding exclusivity and penalizing context bleeding, while mathematically guaranteeing a zero score for static, instruction-ignoring videos.

Our core contributions are summarized as follows:
\begin{itemize}
    \item We systematize the critical failure modes of T2MSV: the \textbf{Static Trap}, \textbf{Context Bleeding}, and \textbf{Identity Amnesia}, highlighting how legacy metrics inadvertently reward these failures.
    \item We propose \textbf{DIAL}, a benchmark featuring 1,000 stress-test scenarios, alongside a 4D decoupled evaluation pipeline.
    \item We introduce \textbf{Diagonal Semantic Alignment (DSA)}, a mathematically rigorous metric that utilizes contrastive normalization to penalize context bleeding and ensure a zero score for static generation.
    \item We conduct an exhaustive evaluation of 9 state-of-the-art models, uncovering the \textbf{``Double-Kill'' Pareto frontier} and establishing a diagnostic foundation for next-generation decoupled video priors.
\end{itemize}
