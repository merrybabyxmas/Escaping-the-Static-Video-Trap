\section{Introduction}
\label{sec:intro}

The field of video generation has transitioned from synthesizing blurry, single-shot clips to attempting complex, cinematic multi-shot narratives (Text-to-Multi-Shot Video, T2MSV). However, evaluation methodologies remain anchored in a single-shot mindset. Existing metrics, such as Fr√©chet Video Distance (FVD) or frame-wise CLIP similarity, evaluate videos holistically. In multi-shot environments, this holistic approach creates a fatal vulnerability: it inadvertently rewards models that generate entirely static, unchanging videos, even when prompted to execute dynamic scene transitions.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/fig1_teaser_real.pdf}
    \caption{The dilemma of current T2MSV models. Existing models typically fall into one of two failure modes given multi-shot prompts requiring semantic shifts: (A) The "Static Trap," where models maintain high consistency scores by generating near-static imagery, ignoring background change instructions. (B) "Amnesia," where models successfully change backgrounds but fail to preserve subject identity. Our benchmark is the first to quantify these distinct failure modes decoupling identity from dynamics.}
    \label{fig:teaser}
\end{figure}

We define this phenomenon as the \textbf{Static Video Trap}. When a prompt demands a sequence jumping from a "rainforest" to "outer space," a model that stubbornly remains in the rainforest will score artificially high on traditional temporal consistency metrics. This creates the \textbf{Global Similarity Paradox}: metrics reward the failure to transition. Conversely, models attempting to dynamically execute the transition often suffer from \textbf{Identity Amnesia}, losing the features of the main subject.

In this work, we argue that T2MSV requires the precise management of \textit{intended discontinuities}. We introduce \textbf{Dynamic-MSV-Bench}, a comprehensive evaluation framework that disentangles subject identity from environmental dynamics across two critical tracks:
\begin{itemize}
    \item \textbf{Track S (Semantic Leap):} Tests narrative diversity, requiring radical environment changes while strictly preserving the subject.
    \item \textbf{Track M (Motion Continuity):} Evaluates spatial integrity, where the background must remain consistent while the camera executes specific motions.
\end{itemize}

Our core contributions are:
\begin{itemize}
    \item We expose the \textbf{Static Trap} and \textbf{Amnesia Trap}, highlighting the flaws of legacy temporal metrics that penalize intended cinematic cuts and semantic shifts.
    \item We propose a 4D decoupled evaluation pipeline, introducing \textbf{Diagonal Semantic Alignment (DSA)}, which assigns a strict zero score to static, instruction-ignoring videos.
    \item We demonstrate that neither Foundation T2V models nor specialized T2MSV inference frameworks have solved the fundamental "Double-Kill" dilemma of multi-shot generation.
\end{itemize}
