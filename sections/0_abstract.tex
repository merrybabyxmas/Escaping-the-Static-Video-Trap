\begin{abstract}
Recent advancements in Text-to-Video (T2V) generation have enabled the creation of visually stunning content. However, as models scale, we observe a paradoxical trend: high temporal consistency scores often hide a lack of true dynamics, a phenomenon we define as the \textbf{Static Video Trap}. In this state, models maintain subject identity by essentially freezing the background, failing to render the scene transitions required by complex, multi-shot narratives. To expose and address this, we introduce \textbf{Dynamic-MSV-Bench}, a comprehensive benchmark suite comprising 1,000 multi-shot scenarios across four dynamic axes: background shift, action sequence, object interaction, and camera motion. We propose a new 4-way evaluation framework—Decoupled Subject-Background Consistency, Diagonal Semantic Alignment, and Cut-Transition Sharpness—that penalizes static behavior and reward narrative fluidity. Our extensive benchmarking of SOTA models reveals that existing architectures are fundamentally biased towards stasis, highlighting a critical bottleneck for the future of cinematic video generation.
\end{abstract}
