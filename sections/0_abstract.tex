\begin{abstract}
As text-to-video (T2V) generation evolves toward sophisticated multi-shot narratives (T2MSV), current evaluation paradigms remain severely constrained by single-shot assumptions. Traditional metrics, such as Fréchet Video Distance (FVD) and frame-wise CLIP similarity, inherently fail to penalize \textbf{``Context Bleeding''}—a phenomenon where models conflate distinct temporal instructions—and inadvertently reward the \textbf{``Static Video Trap,''} where models achieve artificially high temporal consistency by redundantly copying content across shots, completely ignoring narrative transitions. In this paper, we introduce \textbf{DIAL} (Diagonal Instruction ALignment), a comprehensive evaluation benchmark structured into two orthogonal tracks: \textbf{Track S (Semantic Leap)} for drastic environmental shifts, and \textbf{Track M (Motion Continuity)} for fine-grained spatial camera control. Central to our benchmark is \textbf{Diagonal Semantic Alignment (DSA)}. By leveraging a column-wise softmax normalization, DSA shifts the paradigm from absolute alignment to \textbf{Contrastive Instruction Isolation}, explicitly enforcing a zero-sum game that assigns a strict zero score to static, instruction-agnostic generations. Through an exhaustive evaluation of 9 state-of-the-art models across 1,000 stress-test scenarios, we uncover a fundamental \textbf{``Double-Kill'' dilemma}: current monolithic architectures either preserve identity at the expense of dynamics (the Static Trap) or prioritize dynamics but suffer from severe \textbf{Identity Amnesia}. Our findings establish DIAL as a critical diagnostic foundation for developing next-generation decoupled video priors.
\end{abstract}