\begin{abstract}
As text-to-video (T2V) generation transitions toward sophisticated multi-shot narratives, existing evaluation paradigms remain stuck in a single-shot mindset. Traditional metrics like FVD or frame-wise CLIP similarity fail to account for the intentional discontinuities and dynamic transitions required in Text-to-Multi-Shot Video (T2MSV) generation. In this paper, we expose a systematic failure in current state-of-the-art models, termed the \textbf{Static Video Trap}â€”a phenomenon where models achieve high consistency scores by simply copying content across shots, effectively ignoring narrative transitions. To address this, we propose \textbf{Dynamic-MSV-Bench}, a large-scale, two-track benchmark designed to disentangle identity preservation from environmental dynamics. Our framework introduces \textbf{Diagonal Semantic Alignment (DSA)}, a novel metric based on column-wise softmax normalization that mathematically penalizes static content while rewarding shot-specific prompt adherence. Through comprehensive evaluation of 9 SOTA models on 100 diverse multi-shot scenarios, we demonstrate a fundamental "Double-Kill" dilemma: models either preserve identity by sacrificing dynamics (Static Trap) or prioritize dynamics by sacrificing identity (Amnesia Trap). Our findings provide a rigorous foundation for developing next-generation decoupled video priors.
\end{abstract}
