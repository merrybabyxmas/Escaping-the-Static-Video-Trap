\begin{abstract}
As text-to-video (T2V) generation transitions toward sophisticated multi-shot narratives, existing evaluation paradigms remain stuck in a single-shot mindset. Traditional metrics like FVD or frame-wise CLIP similarity fail to account for the intentional discontinuities and dynamic transitions required in Text-to-Multi-Shot Video (T2MSV). In this paper, we expose a systematic failure in current state-of-the-art models, termed the \textbf{Static Video Trap}â€”a phenomenon where models achieve high consistency scores by simply copying content across shots, effectively ignoring narrative transitions. To address this, we propose \textbf{Dynamic-MSV-Bench}, a large-scale benchmark divided into two evaluation tracks: \textbf{Track S (Semantic Leap)} for drastic narrative shifts and \textbf{Track M (Motion Continuity)} for fine-grained camera control. Our framework introduces \textbf{Diagonal Semantic Alignment (DSA)}, a mathematically rigorous metric using column-wise softmax normalization that assigns a strict zero score to static, instruction-ignoring videos. Through comprehensive evaluation of 9 SOTA models on 100 scenarios, we demonstrate a fundamental "Double-Kill" dilemma: models either preserve identity by sacrificing dynamics (Static Trap) or prioritize dynamics by sacrificing identity (Amnesia Trap). Our findings provide a rigorous foundation for next-generation decoupled video priors.
\end{abstract}
