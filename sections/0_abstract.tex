\begin{abstract}
As text-to-video (T2V) generation evolves toward sophisticated multi-shot narratives (T2MSV), current evaluation paradigms remain severely constrained by single-shot assumptions. Traditional metrics, such as FVD and frame-wise CLIP similarity, inherently penalize the intentional discontinuities and dynamic scene transitions essential to cinematic storytelling. In this paper, we expose a systemic vulnerability in state-of-the-art models: the \textbf{Static Video Trap}. In this failure mode, models exploit traditional metrics by redundantly copying content across shots, achieving artificially high temporal consistency while entirely ignoring narrative instructions. To rectify this, we introduce \textbf{Dynamic-MSV-Bench}, a comprehensive evaluation framework structured into two orthogonal tracks: \textbf{Track S (Semantic Leap)} for drastic environmental shifts, and \textbf{Track M (Motion Continuity)} for fine-grained spatial camera control. Central to our benchmark is \textbf{Diagonal Semantic Alignment (DSA)}, a mathematically rigorous metric leveraging column-wise softmax normalization to explicitly assign a zero score to static, instruction-agnostic generations. Through an exhaustive evaluation of 9 SOTA models across 100 stress-test scenarios, we uncover a fundamental "Double-Kill" dilemma: current monolithic architectures either preserve identity at the expense of dynamics (the Static Trap) or prioritize dynamics but suffer from severe \textbf{Identity Amnesia}. Ultimately, our findings expose the architectural entanglement of current video priors and establish a rigorous diagnostic foundation for developing next-generation decoupled dynamics.
\end{abstract}